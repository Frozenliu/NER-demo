{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from src.model import BiLSTM_crf\n",
    "from src.utils import load_data_and_labels,save_pred,transformer_x,transformer_y\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# load word2vec\n",
    "word2vec = {}\n",
    "with open('../data/word2vec.txt','r',encoding = 'utf8') as f:\n",
    "    f.readline() # 第一行是word数量以及embedding size;跳过\n",
    "    for line in f.readlines():\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "# load data and labels\n",
    "x_train, y_train = load_data_and_labels('../data/train.txt')\n",
    "transformer_x = transformer_x()\n",
    "x_train = transformer_x.fit(x_train)\n",
    "vocab_size = transformer_x.vocab_size\n",
    "transformer_y = transformer_y(transformer_x.max_len)\n",
    "y_train = transformer_y.to_onehot(y_train)\n",
    "\n",
    "#init embedding_layer\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "word2id = transformer_x.word2id\n",
    "for word,vec in word2vec.items():\n",
    "    if word in word2id:\n",
    "        embedding_matrix[word2id[word]] = word2vec[word]\n",
    "\n",
    "model = BiLSTM_crf(num_labels=8,embedding_matrix =embedding_matrix ,max_seq_len=transformer_x.max_len,use_crf = True)\n",
    "model = model.build()\n",
    "model.summary()\n",
    "\n",
    "x_test, y_test = load_data_and_labels('../data/dev.txt')\n",
    "x_test = transformer_x.tran(x_test)\n",
    "y_test_onehot = transformer_y.to_onehot(y_test)\n",
    "model.fit(x_train,y_train,validation_data = (x_test,y_test_onehot),verbose = 1,batch_size = 12,epochs= 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "pred = transformer_y.to_tag(pred)\n",
    "print(metrics.flat_f1_score(y_test, pred,\n",
    "                      average='weighted', labels=transformer_y.tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group B and I results\n",
    "labels = transformer_y.tags\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('../data/pretrain_embedding')\n",
    "save_path = '../data/pretrain_embedding/pred.txt'\n",
    "save_pred(pred,save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
